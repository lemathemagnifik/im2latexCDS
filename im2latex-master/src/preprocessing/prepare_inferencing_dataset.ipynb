{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex\n",
    "\n",
    "&copy; Copyright 2017 - 2018 Sumeet S Singh\n",
    "\n",
    "    This file is part of im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Create dataset files for inferencing\n",
    "\n",
    "This notebook creates data files that can be used for inferencing.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "apJbCsBHl-2A",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, Image as ipImage\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves import cPickle as pickle\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pd.options.display.max_rows = 600\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.width = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Arguments\n",
    "Create a new 'raw_data_dir' - e.g. \"inferencing\" - in your data_dir (alongside the folders step1, step2 etc.). Copy the image files who's LaTeX you want to generate into the formula_images folder. Next supply the name of image files below and modify any other arguments you may want below. For inferencing, execute run.py under the --test flag and with --raw-data-dir option pointing to the abovesaid folder. \n",
    "\n",
    "You'll need to supply the correct per-gpu batch-size value (-b flag) to run.py. The overall batch-size HYPER_batch_size = per-gpu-batch-size * num_gpus. HYPER_batch_size is set to number of images by default below, which means that when you execute run.py, you need to supply a per-gpu batch-size (using the -b option) = HYPER_batch_size/num_gpus or an integral factor of it. For e.g. if you had 50 images to infer, and had 1 GPU, you could supply \"-b 50\", \"-b 25\", \"-b 10\" etc. If you had 2 GPUs, then you can specify \"-b 25\" and \"-b 5\" in this case. You can also set the value of HYPER_batch_size to a factor of len(images) as long as you ensure that HYPER_batch_size is a factor of len(images) - add, remove or duplicate images from/to your inferencing set to get suitable total size. So for e.g. if you had 2 GPUs and 998 images to infer, you could duplicate two images (giving them different names) to get a total number of 1000 and then set HYPER_batch_size = 50 and supply \"-b 25\" option to run.py.\n",
    "\n",
    "Finally, after you're convinced that everything is working, switch the flag \"dump\" below to True and rerun the notebook. The flag defaults to False, meaning that it won't write files to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ['0000a586456794e_basic.png', '0000a8416b30429_basic.png', '0000ca7c3d3830b_basic.png', '00021a10c3d0ffc_basic.png']\n",
    "HYPER_batch_size = len(images)\n",
    "data_folder = data_dir = '../data/dataset5'\n",
    "image_folder = image_dir = os.path.join(data_folder, 'formula_images')\n",
    "raw_data_dir = os.path.join(data_folder, 'inferencing_%d'%HYPER_batch_size)\n",
    "dump = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dir = ../data/dataset5/inferencing_4\n"
     ]
    }
   ],
   "source": [
    "print('Output Dir = %s'%raw_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>word2id_len</th>\n",
       "      <th>bin_len</th>\n",
       "      <th>word2id</th>\n",
       "      <th>padded_seq</th>\n",
       "      <th>padded_seq_len</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>squashed_len</th>\n",
       "      <th>squashed_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000a586456794e_basic.png</td>\n",
       "      <td>82</td>\n",
       "      <td>615</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000a8416b30429_basic.png</td>\n",
       "      <td>30</td>\n",
       "      <td>264</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000ca7c3d3830b_basic.png</td>\n",
       "      <td>78</td>\n",
       "      <td>360</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00021a10c3d0ffc_basic.png</td>\n",
       "      <td>79</td>\n",
       "      <td>882</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  height  width  word2id_len  bin_len  \\\n",
       "0  0000a586456794e_basic.png      82    615          150      151   \n",
       "1  0000a8416b30429_basic.png      30    264          150      151   \n",
       "2  0000ca7c3d3830b_basic.png      78    360          150      151   \n",
       "3  00021a10c3d0ffc_basic.png      79    882          150      151   \n",
       "\n",
       "                                                                                               word2id  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                                                                            padded_seq  padded_seq_len  seq_len  squashed_len  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...             151      151           151   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...             151      151           151   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...             151      151           151   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...             151      151           151   \n",
       "\n",
       "                                                                                          squashed_seq  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {'image':[], 'height':[], 'width':[], 'word2id_len':[], u'bin_len':[], u'word2id':[], u'padded_seq':[], u'padded_seq_len':[], u'seq_len':[], u'squashed_len':[], u'squashed_seq':[]}\n",
    "df_train_squashed = pd.DataFrame(data_dict)[['image', 'height', 'width', 'word2id_len', u'bin_len', u'word2id', u'padded_seq', u'padded_seq_len', u'seq_len', u'squashed_len', u'squashed_seq']]\n",
    "df_valid_squashed = pd.DataFrame(data_dict)[['image', 'height', 'width', 'word2id_len', u'bin_len', u'word2id', u'padded_seq', u'padded_seq_len', u'seq_len', u'squashed_len', u'squashed_seq']]\n",
    "\n",
    "l = 150\n",
    "word2id = [1]*l\n",
    "padded_seq = word2id + [0]\n",
    "padded_len = l + 1\n",
    "for f in images:\n",
    "    im = Image.open(os.path.join(image_folder,f))\n",
    "    w = im.size[0]\n",
    "    h = im.size[1]\n",
    "    data_dict['image'].append(f)\n",
    "    data_dict['height'].append(h)\n",
    "    data_dict['width'].append(w)\n",
    "    data_dict['word2id_len'].append(l)\n",
    "    data_dict['bin_len'].append(padded_len)\n",
    "    data_dict['word2id'].append(word2id)\n",
    "    data_dict['padded_seq'].append(padded_seq)\n",
    "    data_dict['padded_seq_len'].append(padded_len)\n",
    "    data_dict['seq_len'].append(padded_len)\n",
    "    data_dict['squashed_len'].append(padded_len)\n",
    "    data_dict['squashed_seq'].append(padded_seq)\n",
    "df_test_squashed = pd.DataFrame(data_dict)[['image', 'height', 'width', 'word2id_len', u'bin_len', u'word2id', u'padded_seq', u'padded_seq_len', u'seq_len', u'squashed_len', u'squashed_seq']]\n",
    "df_test_squashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>word2id_len</th>\n",
       "      <th>bin_len</th>\n",
       "      <th>word2id</th>\n",
       "      <th>padded_seq</th>\n",
       "      <th>padded_seq_len</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>squashed_len</th>\n",
       "      <th>squashed_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [image, height, width, word2id_len, bin_len, word2id, padded_seq, padded_seq_len, seq_len, squashed_len, squashed_seq]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_squashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_bins(df_):\n",
    "    \"\"\"\n",
    "    Creates ndarrays of (padded) sequence bins from df_*_squashed / df_*_padded \n",
    "    and pickles them as a dictionary of ndarrays wrapped in dataframes.\n",
    "    This preprocessing is needed in order to quickly obtain an ndarray of\n",
    "    token-sequences at training time.\n",
    "    \"\"\"\n",
    "    bin_lens = df_.bin_len.unique()\n",
    "    bins = {}\n",
    "    bins_squashed = {}\n",
    "    \n",
    "    for len_ in bin_lens:\n",
    "        df_slice = df_[df_.padded_seq_len == len_]\n",
    "        bin_ = np.array(df_slice.padded_seq.values.tolist(), dtype=np.int32)\n",
    "        bin_squashed = np.array(df_slice.squashed_seq.values.tolist(), dtype=np.int32)\n",
    "        assert bin_.shape[1] == len_\n",
    "        assert bin_.shape[0] == df_slice.shape[0]\n",
    "        bins[len_] = pd.DataFrame(bin_, index=df_slice.index)\n",
    "        bins_squashed[len_] = pd.DataFrame(bin_squashed, index=df_slice.index)\n",
    "    return bins, bins_squashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_test, bins_sq_test = make_seq_bins(df_test_squashed)\n",
    "bins_train, bins_sq_train = make_seq_bins(df_train_squashed)\n",
    "bins_valid, bins_sq_valid = make_seq_bins(df_valid_squashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dump:\n",
    "    if not os.path.exists(raw_data_dir):\n",
    "        os.makedirs(raw_data_dir)\n",
    "    with open(os.path.join(raw_data_dir, 'batch_size.pkl'), 'wb') as f:\n",
    "      pickle.dump(HYPER_batch_size, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(raw_data_dir, 'batch_size.pkl'), 'rb') as f:\n",
    "        assert pickle.load(f) == HYPER_batch_size\n",
    "        \n",
    "    df_train_squashed.to_pickle(os.path.join(raw_data_dir, 'df_train.pkl'))\n",
    "    df_test_squashed.to_pickle(os.path.join(raw_data_dir, 'df_test.pkl'))\n",
    "    df_valid_squashed.to_pickle(os.path.join(raw_data_dir, 'df_valid.pkl'))\n",
    "    \n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_train.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_sq_train.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_sq_train, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_test.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_sq_test.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_sq_test, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_valid.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_valid, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(raw_data_dir, 'raw_seq_sq_valid.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins_sq_valid, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    data_props = {}\n",
    "    dict_vocab = pd.read_pickle(os.path.join(data_dir, 'step2', 'dict_vocab.pkl'))\n",
    "    word2id = dict_vocab['id']\n",
    "    id2word = pd.read_pickle(os.path.join(data_dir, 'step2', 'dict_id2word.pkl'))\n",
    "    data_props['id2word'] = id2word\n",
    "    data_props['word2id'] = word2id\n",
    "    data_props['K'] = max(id2word.keys()) + 1\n",
    "    data_props['SpaceTokenID'] = word2id[' '] if ' ' in word2id else None\n",
    "    data_props['NullTokenID'] = word2id[r'\\eos']\n",
    "    data_props['StartTokenID'] = word2id[r'\\bos']\n",
    "    data_props['MaxSeqLen'] = df_test_squashed.padded_seq_len.max()\n",
    "    padded_image_dim = pd.read_pickle(os.path.join(data_dir, 'step3', 'padded_image_dim.pkl'))\n",
    "    data_props['padded_image_dim'] = {'height': padded_image_dim['height'], 'width':padded_image_dim['width']}\n",
    "    with open(os.path.join(raw_data_dir, 'data_props.pkl'), 'wb') as f:\n",
    "        pickle.dump(data_props, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an inferencing cycle\n",
    "After you've generated the dataset as above and have a trained model snapshot, execute run.py using the --test flag and with --raw-data-folder pointing to the abovementioned directory and providing model snapshot via. the --restore option. Also remember to adjust the per-gpu batch-size (-b flag) appropriately e.g.: ./run.py -a 0.0001 -e -1 -b 50 -v -1 -i 2 --r-lambda 0.00005 --raw-data-folder ../data/dataset5/inferencing_100 --test --save-all-eval --restore \"./tb_metrics/2017-12-25 21-04-15 PST 140K_noRegroup_score89.09/test_runs/step_00167526_temp\".\n",
    "\n",
    "See the notebook visualize.ipynb for examples on how to extract the predictions fro the underlying h5py files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
